# AUTOGENERATED! DO NOT EDIT! File to edit: nbks/03_train.ipynb (unless otherwise specified).

__all__ = ['train', 'train_cv']

# Cell
from .dataset import *
from .evaluate import *

from fastai.vision.all import *
from fastcore.script import *
from fastai.callback.wandb import *
from wwf.vision.timm import *
import timm
import wandb
from typing import *
from sys import exit

# Cell
def train(
    epochs: int, lr: Union[float, str], frz: int=1, pre: int=800, re: int=256,
    bs: int=256, fold: int=4, smooth: bool=False,
    arch: str='resnet18', dump: bool=False, log: bool=False, mixup: float=0.,
    fp16: bool=False, dls: DataLoaders=None,
 ):
    # Prep Data, Opt, Loss, Arch
    if dls is None: dls = get_dls_all_in_1(presize=pre, resize=re, bs=bs, val_fold=fold)
    if log: wandb.init(project="plant-pathology")
    if smooth: loss_func = LabelSmoothingCrossEntropyFlat()
    else:      loss_func = CrossEntropyLossFlat()
    m, learner_func = timm_or_fastai_arch(arch)

    # Add callbacks
    cbs = [WandbCallback(), SaveModelCallback()] if log else []
    if mixup: cbs.append(MixUp(mixup))

    # Build learner
    learn = learner_func(dls, m, loss_func=loss_func,
                    metrics=[accuracy, RocAuc()], cbs=cbs)
    if dump: print(learn.model); exit()
    if lr=="find": learn.lr_find(); exit()
    if fp16: learn.to_fp16()

    # Train
    learn.freeze()
    learn.fit_one_cycle(frz, lr)
    learn.unfreeze()
    learn.fit_one_cycle(epochs, slice(lr/100, lr/2))  # Explore other divs
    return learn

# Cell
@call_parse
def train_cv(
    epochs:   Param("Number of unfrozen epochs", int),
    lr:       Param("Initial learning rate", float),
    frz:      Param("Number of frozen epochs", int)=1,
    pre:      Param("Presize", int)=800,
    re:       Param("Resize", int)=256,
    bs:       Param("Batch size", int)=256,
    smooth:   Param("Label smoothing?", store_true)=False,
    arch:     Param("Architecture", str)='resnet18',
    dump:     Param("Print model", store_true)=False,
    log:      Param("Log w/ W&B", store_true)=False,
    mixup:    Param("Mixup", float)=0.0,
    tta:      Param("Test-time augmentation", store_true)=False,
    fp16:     Param("Use mixed-precision", store_true)=False,
    eval_dir: Param("Evaluate model, save results in dir", Path)=None,
):
    print(locals())
    scores = []
    for fold in range(5):
        print(f"\nTraining on fold {fold}")
        learn = train(epochs, lr, frz=frz, pre=pre, re=re, bs=bs, smooth=smooth,
                      arch=arch, dump=dump, log=log, fold=fold, mixup=mixup,
                      fp16=fp16,)
        if tta:
            preds, lbls = learn.tta()
            res = [f(preds, lbls) for f in [learn.loss_func, accuracy, RocAuc()]]
        else: res = learn.final_record
        scores.append(res)

        # Create submission file for this model
        if eval_dir: evaluate(learn, Path(eval_dir)/f"predictions_fold_{fold}.csv", tta=True)

        # Delete learner to avoid OOM
        del learn
    scores = np.array(scores)
    print(f"Scores: {scores}\n")
    print(f"Mean: {scores.mean(0)}")