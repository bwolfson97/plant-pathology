{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "> This module contains a script to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from plant_pathology.dataset import *\n",
    "from plant_pathology.evaluate import *\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastcore.script import *\n",
    "from fastai.callback.wandb import *\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model on Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frz,epochs,pre,re,bs,fold,smooth,arch,dump,log,lr = 1,1,64,31,512,0,True,\"resnet18\",False,False,1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls_all_in_1(presize=pre, resize=re, bs=bs, val_fold=fold)\n",
    "if smooth: loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "else:      loss_func = CrossEntropyLossFlat()\n",
    "m = globals()[arch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Learner\n",
    "cbs = [WandbCallback(), SaveModelCallback()] if log else []\n",
    "learn = Learner(dls, m, loss_func=loss_func,\n",
    "                metrics=[accuracy, RocAuc()], cbs=cbs)\n",
    "if dump: print(learn.model); exit()\n",
    "# Add more callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(epochs: int, lr: float, frz: int=1, pre: int=800, re: int=256,\n",
    "          bs: int=256, fold: int=0, smooth: bool=True, \n",
    "          arch: str='resnet18', dump: bool=False, log: bool=False):\n",
    "    # Prep Data, Opt, Loss, Arch\n",
    "    if log: wandb.init(project=\"plant-pathology\")\n",
    "    dls = get_dls_all_in_1(presize=pre, resize=re, bs=bs, val_fold=fold)\n",
    "    if smooth: loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "    else:      loss_func = CrossEntropyLossFlat()\n",
    "    m = globals()[arch]\n",
    "    \n",
    "    # Build Learner\n",
    "    cbs = [WandbCallback(), SaveModelCallback()] if log else []\n",
    "    learn = Learner(dls, create_cnn_model(m, dls.c), loss_func=loss_func,\n",
    "                    metrics=[accuracy, RocAuc()], cbs=cbs)\n",
    "    if dump: print(learn.model); exit()\n",
    "    # Add more callbacks\n",
    "    \n",
    "    print(f\"Training for {frz} epochs frozen, {epochs} unfrozen\")\n",
    "    learn.freeze()\n",
    "    learn.fit_one_cycle(frz, lr)\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(epochs, slice(lr/100, lr/2))  # Explore other divs\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = train(0, 0.001, bs=256, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.final_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def train_cv(\n",
    "    epochs: Param(\"Number of unfrozen epochs\", int), \n",
    "    lr:     Param(\"Initial learning rate\", float), \n",
    "    frz:    Param(\"Number of frozen epochs\", int)=1, \n",
    "    pre:    Param(\"Presize\", int)=800, \n",
    "    re:     Param(\"Resize\", int)=256,\n",
    "    bs:     Param(\"Batch size\", int)=256,  \n",
    "    smooth: Param(\"Label smoothing?\", bool_arg)=False, \n",
    "    arch:   Param(\"Architecture\", str)='resnet18', \n",
    "    dump:   Param(\"Print model\", bool_arg)=False, \n",
    "    log:    Param(\"Log w/ W&B\", bool_arg)=False,\n",
    "):\n",
    "    scores = []\n",
    "    for fold in range(5):\n",
    "        learn = train(epochs, lr, frz=frz, pre=pre, re=re, bs=bs, smooth=smooth, arch=arch, dump=dump, log=log, fold=fold)\n",
    "        scores.append(learn.final_record)\n",
    "        \n",
    "        # Create submission file for this model\n",
    "        evaluate(learn, f\"predictions_fold_{fold}.csv\")\n",
    "    scores = np.array(scores)\n",
    "    print(f\"Scores: {scores}\")\n",
    "    print(f\"Mean: {scores.mean(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.ones((5, 4))\n",
    "scores.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
